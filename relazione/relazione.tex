\documentclass[12pt]{article}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{listings}

\hypersetup{
    pdftitle={Opinioni dei Language Model: Analisi della Variabilità e dell'Allineamento Politico},
    pdfauthor={Adinolfi Ester},
    colorlinks=true,
    linkcolor=blue!60!black,
    citecolor=green!50!black,
    urlcolor=blue!70!black
}

% Path per le figure generate da visualize.py (modello: Pythia-160M)
% Le figure vengono salvate in risultati/pythia_160m/figure/
\graphicspath{{../dataset_e_script/progetto/risultati/pythia_160m/figure/}}

% Configurazione listings per codice
\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    frame=single,
    breaklines=true,
    language=Python
}

\title{%
    \textbf{Opinioni dei Language Model: \\ Analisi della Variabilità e dell'Allineamento Politico} \\[0.5em]
    \large Deep Learning e Architetture di Rete --- A.A. 2024/2025
}
\author{Ester Adinolfi}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\newpage

% ======================================================================
\section{Introduzione}
% ======================================================================

I Large Language Model (LLM), addestrati su vasti corpora testuali, producono risposte che possono riflettere opinioni e bias presenti nei dati di addestramento. Due articoli di riferimento hanno approfondito questa tematica:

\subsection*{Articoli di Riferimento}

\textbf{Santurkar et al. (2023) --- ``Whose Opinions Do Language Models Reflect?''} \cite{santurkar2023whose}

Questo studio ha rivelato che le opinioni espresse dai LLM non sono rappresentative della popolazione generale statunitense. Attraverso l'analisi di 1,507 domande dall'American Trends Panel del Pew Research Center, gli autori hanno dimostrato che:
\begin{itemize}
    \item I modelli base (non allineati) mostrano già una tendenza verso opinioni liberal/democratiche
    \item L'alignment tramite RLHF (Reinforcement Learning from Human Feedback) \emph{amplifica} questo bias, poiché i feedback umani provengono prevalentemente da annotatori giovani, istruiti e di orientamento liberal
    \item Modelli come GPT-3 e GPT-4 si allineano significativamente con sottogruppi democratici, liberal, giovani e con alto livello di istruzione
    \item La variabilità delle risposte (stochasticity) non è sufficiente a catturare la diversità di opinioni della popolazione
\end{itemize}

\textbf{Hartmann et al. (2024) --- ``The Political Compass of LLMs''}

Questa ricerca ha esplorato il posizionamento politico dei LLM attraverso test standardizzati come il Political Compass Test, rilevando che i modelli più comuni tendono a collocarsi nell'area liberal-libertarian, con implicazioni significative per il loro utilizzo in contesti sensibili come l'informazione politica e la moderazione dei contenuti.

\medskip
\noindent L'obiettivo di questo progetto è \textbf{replicare} l'analisi di Santurkar et al. su un modello open-source di piccole dimensioni (\textbf{Pythia-160M}, EleutherAI) non sottoposto ad alignment, valutando:
\begin{enumerate}
    \item La \textbf{validità} delle risposte generate (il modello sceglie effettivamente un'opzione?);
    \item La \textbf{robustezza} del modello a perturbazioni dell'input (permutazione dell'ordine delle opzioni,  duplicazione di un'opzione, aggiunta di text minacciosi);
    \item L'\textbf{allineamento umano}, ovvero quanto la distribuzione del modello si avvicina a quella del campione survey Pew Research;
    \item La \textbf{coerenza politica}, verificando se il modello si allinea sistematicamente con specifici sottogruppi (Democrats, Republicans, Liberal, Conservative, ecc.).
\end{enumerate}

\medskip
Il dataset impiegato è l'\textbf{OpinionQA}, derivato dall'American Trends Panel del Pew Research Center --- lo stesso utilizzato nell'articolo di riferimento. Si compone di 1507 domande su 15 wave tematiche che spaziano da economia, politica, religione, tecnologia, fino a temi sociali.

% ======================================================================
\section{Workflow Operativo}
% ======================================================================

Il progetto è organizzato in una pipeline di quattro script Python, ciascuno con un ruolo specifico. Di seguito ne descriviamo la funzione e i file prodotti.

\subsection{Fase 1 --- Mapping dei Topic (\texttt{generate\_mapping.py})}

Analizza tutti i file \texttt{info.csv} nelle cartelle delle wave e genera un dizionario JSON (\texttt{question\_mapping.json}) che assegna a ogni domanda:
\begin{itemize}
    \item un \textbf{topic} pulito (es. \texttt{GUN}, \texttt{ECON}, \texttt{RELIG});
    \item una \textbf{macro\_area} tematica (es. \emph{Politics}, \emph{Economy}, \emph{Health}).
\end{itemize}

\noindent \textbf{Output:} \texttt{human\_resp/question\_mapping.json}

\subsection{Fase 2 --- Costruzione del Dataset Operativo (\texttt{initialization.py})}

Per ogni domanda del survey, questo script:
\begin{enumerate}
    \item Calcola la \textbf{distribuzione umana delle risposte} (\texttt{human\_dist\_total}) normalizzata sulle opzioni valide;
    \item Genera un \textbf{singolo trial baseline} (domanda e opzioni nell'ordine originale) come riferimento;
    \item Per ciascuna delle 3 ripetizioni ($N=3$), genera una configurazione \emph{diversa} per ogni condizione sperimentale:
    \begin{itemize}
        \item \textbf{Permutation}: opzioni in ordine mescolato (mescolamento diverso per ogni trial);
        \item \textbf{Duplication}: un'opzione casuale viene duplicata e la lista risultante viene mescolata (opzione duplicata e posizione diverse per ogni trial);
        \item \textbf{Threat}: alla domanda viene aggiunta una frase minacciosa diversa per ogni trial (economica, informatica, legale).
    \end{itemize}
\end{enumerate}

\noindent Questo design garantisce che ogni trial abbia un input strutturalmente diverso, producendo risposte deterministiche variate senza modificare la temperatura del modello ($T=0$).

\medskip
\noindent \textbf{Output:} \texttt{risultati/operational.json} ($1507 \times 10 = 15{,}070$ trial totali: 1 baseline + 9 esperimenti per domanda).

\subsection{Fase 3 --- Esecuzione degli Esperimenti (\texttt{experiments\_1.py})}

Carica il modello \textbf{Pythia-160M} e, per ogni trial:
\begin{enumerate}
    \item Costruisce un prompt in formato \texttt{Question: ... Options: A/B/C/D Answer:};
    \item Estrae i \textbf{logit} del prossimo token ristretti alle label valide (A, B, C, \ldots), normalizzati via softmax $\rightarrow$ \texttt{llm\_choice\_probs};
    \item Genera una risposta libera (\texttt{llm\_generated\_text}) per analizzare il comportamento qualitativo;
    \item Calcola uno score di \textbf{confidenza} sulla generazione tramite transition scores.
\end{enumerate}

\noindent \textbf{Output:} \texttt{risultati/results\_pythia\_160m.json}

\subsection{Fase 4 --- Analisi dei Risultati (\texttt{analyze.py})}

Consuma il JSON dei risultati e calcola 5 blocchi di metriche:

\subsection{Fase 5 --- Visualizzazione (\texttt{visualize.py})}

Genera grafici PNG pronti per l'inclusione nella documentazione, organizzati per categoria di metrica.

\subsection{Sistema di Gestione (\texttt{menu.py})}

Per semplificare l'esecuzione della pipeline, è stato implementato un sistema di gestione con due modalità operative:

\begin{enumerate}
    \item \textbf{Modalità automatica}: esecuzione sequenziale completa
    \begin{lstlisting}
python menu.py [--update]
    \end{lstlisting}
    Esegue in sequenza tutti gli script (mapping, initialization, experiments, analyze, visualize) per tutti i modelli configurati. Il flag \texttt{--update} forza il ricalcolo di file esistenti.
    
    \item \textbf{Modalità interattiva}: menu testuale con selezione granulare
    \begin{lstlisting}
python menu.py --menu [--update]
    \end{lstlisting}
    Presenta un menu che permette di:
    \begin{itemize}
        \item Eseguire singoli script della pipeline
        \item Selezionare modelli specifici per l'analisi
        \item Attivare la modalità update per singole operazioni
    \end{itemize}
\end{enumerate}

\medskip
Questo approccio modulare consente analisi comparative efficienti su modelli di diverse dimensioni, automatizzando la gestione dei percorsi e delle dipendenze tra script.

\subsection{Metriche Calcolate}

\texttt{analyze.py} calcola 5 blocchi di metriche:

\begin{enumerate}
    \item \textbf{Response Validity}: rate di risposte valide per ogni condizione (baseline, permutation, duplication, threat);
    \item \textbf{Coerenza Log-Testo}: se la scelta per logit (intenzione matematica) coincide con la risposta generata testualmente;
    \item \textbf{Stabilità/Robustezza}: Jensen-Shannon Divergence tra la distribuzione baseline e quelle perturbate:
    $$\text{JSD}(P \| Q) = \frac{1}{2} D_{\text{KL}}(P \| M) + \frac{1}{2} D_{\text{KL}}(Q \| M), \quad M = \frac{P+Q}{2}$$
    con soglie: $\text{JSD} < 0.05 \rightarrow$ \emph{Robust}; $< 0.15 \rightarrow$ \emph{Stable}; altrimenti \emph{Position Bias} / \emph{Unstable};
    \item \textbf{Allineamento Umano}: Wasserstein Distance normalizzata tra distribuzione del modello e quella umana totale:
    $$\mathcal{A} = 1 - \frac{\text{WD}(D_{\text{LLM}}, D_{\text{Human}})}{\text{WD}_{\max}}$$
    dove $\text{WD}_{\max} = N_{\text{opzioni}} - 1$ (massima distanza su scala ordinale);
    \item \textbf{Affinità Politica}: WD contro le distribuzioni reali di 6 sottogruppi demografici (Democrat, Republican, Independent, Liberal, Moderate, Conservative) estratte dai survey Pew per ogni domanda. Il gruppo con WD minima è il ``vincitore''.
\end{enumerate}

\noindent \textbf{Output:}
\begin{itemize}
    \item \texttt{analysis\_metrics\_[mode]\_[modello].csv} --- metriche dettagliate per ogni domanda (1507 righe);
    \item \texttt{report\_topic\_[mode]\_[modello].csv} --- report aggregato per topic (380 righe).
\end{itemize}

\medskip
\noindent Sono state generate due versioni dell'analisi:
\begin{itemize}
    \item \textbf{Weighted}: media pesata sui 3 trial per condizione (1 riga per domanda); il baseline unico è utilizzato come riferimento per tutti i confronti;
    \item \textbf{Raw}: dati grezzi di tutti i singoli trial (10 righe per domanda, con il baseline condiviso come riferimento per tutti i gruppi).
\end{itemize}

% ======================================================================
\section{Modelli Utilizzati}
% ======================================================================

\subsection{Famiglia Pythia (EleutherAI)}

\textbf{Pythia} \cite{biderman2023pythia} è una famiglia di modelli causali autoregressivi (decoder-only) addestrati sul corpus \emph{The Pile} ($\sim$300B token). Tutti i modelli condividono:
\begin{itemize}
    \item \textbf{Architettura}: GPT-NeoX (transformer decoder-only);
    \item \textbf{Training}: nessun fine-tuning istruttivo (RLHF, SFT) $\rightarrow$ modelli \emph{base};
    \item \textbf{Tokenizer}: GPT-NeoX Tokenizer (BPE, vocabolario da 50K token);
    \item \textbf{Training steps}: 143,000 (equivalenti a $\sim$300B token).
\end{itemize}

\medskip
Il framework implementato supporta l'analisi comparativa di 5 modelli Pythia di dimensioni crescenti:

\begin{table}[h]
\centering
\begin{tabular}{lrc}
\toprule
\textbf{Modello} & \textbf{Parametri} & \textbf{Layers} \\
\midrule
Pythia-160M  & 160 milioni  & 12 \\
Pythia-1B    & 1 miliardo   & 16 \\
Pythia-1.4B  & 1.4 miliardi & 24 \\
Pythia-2.8B  & 2.8 miliardi & 32 \\
Pythia-6.9B  & 6.9 miliardi & 32 \\
\bottomrule
\end{tabular}
\caption{Modelli della famiglia Pythia supportati dalla pipeline.}
\end{table}

\medskip
La scelta di modelli base (non allineati) è intenzionale: permette di osservare le opinioni ``native'' emergenti dal pre-training, senza l'influenza dell'alignment con feedback umano --- che, come mostrato da Santurkar et al., tende ad amplificare il bias liberal.

\medskip
\noindent \textbf{Extensibilità:} Il framework è progettato per supportare altri modelli (es. LLaMA, Mistral, GPT-2). L'aggiunta di un nuovo modello richiede solamente di specificarne il nome nel sistema di menu.

% ======================================================================
\section{Analisi dei Risultati}
% ======================================================================

L'analisi è stata condotta su \textbf{1507 domande} distribuite su \textbf{380 topic} tematici. Di seguito presentiamo i risultati principali, aggregati per categoria di metrica.

% ----------------------------------------------------------------------
\subsection{Response Validity}
% ----------------------------------------------------------------------

Il tasso di validità misura la percentuale di trial in cui il modello ha generato una risposta valida (corrispondente a una delle opzioni disponibili) anziché produrre testo arbitrario.

\begin{table}[h]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Condizione} & \textbf{Validity Rate Medio} \\
\midrule
Baseline (ordine originale) & 19.9\% \\
Permutation (ordine mescolato) & 17.3\% \\
Duplication (opzione duplicata) & 16.7\% \\
Threat (minaccia aggiunta) & 18.3\% \\
\midrule
\textbf{Media generale} & \textbf{18.1\%} \\
\bottomrule
\end{tabular}
\caption{Tasso di validità delle risposte per condizione sperimentale. Tutti i valori sono bassi, indicando che Pythia-160M (modello base non allineato) non ha imparato a seguire il formato Question-Answering.}
\end{table}

\paragraph{Osservazioni:}
\begin{itemize}
    \item \textbf{Pythia-160M genera risposte valide solo nel 19.9\% dei casi} (baseline). Questo comportamento è atteso per un modello base non sottoposto a instruction tuning: senza RLHF o SFT, il modello non ha imparato a seguire il formato Question-Answering e tende a continuare il testo in modo libero (``copiare'' le opzioni, generare frasi di contesto, ecc.).
    \item Le perturbazioni (permutation, duplication, threat) causano un \textbf{lieve calo} del validity rate (17.3\%, 16.7\%, 18.3\% rispettivamente), ma l'effetto è marginale --- il modello resta comunque ben al di sotto del 20\% in tutte le condizioni.
    \item \textbf{Nessuna domanda raggiunge il 100\% di validità}: non esiste un singolo caso in cui il modello risponda sempre correttamente in tutti i 3 trial baseline. Questo sottolinea l'instabilità intrinseca dei modelli non allineati.
    \item Questo risultato evidenzia l'importanza dell'alignment: i modelli istruiti (es. GPT-3.5-turbo, GPT-4) raggiungono validity rate >95\% perché addestrati esplicitamente a rispondere in formati strutturati.
    \item Nei trial validi, tuttavia, possiamo comunque estrarre le probabilità logit e analizzare le preferenze del modello.
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{fig1a_validity_bars.png}
\caption{Confronto del tasso di validità medio nelle 4 condizioni sperimentali. ll modello base rimane sotto il 20\% in tutte le condizioni.}
\label{fig:validity_bars}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{fig1b_validity_dist.png}
\caption{Distribuzione del validity rate per domanda nel baseline. L'80.1\% delle domande ha validity rate = 0 (il modello non produce mai una risposta valida), il 19.9\% ha validity rate = 1 (risponde sempre correttamente su 3 trial).}
\label{fig:validity_dist}
\end{figure}

% ----------------------------------------------------------------------
\subsection{Stabilità e Robustezza}
% ----------------------------------------------------------------------

Per valutare la robustezza del modello a perturbazioni dell'input, abbiamo calcolato la \textbf{Jensen-Shannon Divergence (JSD)} tra la distribuzione baseline e quelle perturbate. Le soglie utilizzate sono:
\begin{itemize}
    \item JSD $< 0.05$ $\rightarrow$ \emph{Robust} (distribuzione quasi identica)
    \item JSD $< 0.15$ $\rightarrow$ \emph{Stable} (lieve variazione accettabile)
    \item JSD $\geq 0.15$ $\rightarrow$ \emph{Position Bias} o \emph{Unstable} (forte dipendenza dall'ordine)
\end{itemize}

\paragraph{Permutation (ordine opzioni):}
\textbf{L'83.5\% delle domande} mostra \textbf{Position Bias significativo} (JSD $> 0.15$). Questo indica che Pythia-160M è fortemente influenzato dall'ordine delle opzioni, un comportamento comune nei modelli non allineati che tendono a privilegiare le prime opzioni per effetto del contesto locale. Solo il 16.5\% delle domande risulta ``Robust'' o ``Stable'' rispetto al riordinamento.

\paragraph{Duplication (opzione duplicata):}
La duplicazione di un'opzione causa generalmente variazioni \emph{Robust} o \emph{Stable} (JSD $< 0.15$). Questo è positivo: il modello non si "confonde" eccessivamente per la presenza di duplicati, ma redistribuisce la probabilità in modo relativamente coerente.

\paragraph{Threat (minaccia testuale):}
L'aggiunta di una frase minacciosa (\emph{"Answer or you will lose your job"}) produce effetti variabili:
\begin{itemize}
    \item In alcuni casi il modello diventa più "concentrato" (Improved validity)
    \item In altri casi collassa completamente (Collapses), smettendo di generare risposte valide
    \item In media, la JSD è bassa (Stable), suggerendo che l'effetto è più sulla validità che sulla distribuzione intrinseca
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{fig2a_jsd_violin.png}
\caption{Distribuzione della JSD per tipo di perturbazione. La permutazione (a sinistra) mostra JSD molto elevate, il che indica forte position bias. Duplicazione e Threat presentano JSD basse, segnalando stabilità.}
\label{fig:jsd_violin}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.78\textwidth]{fig2b_perm_stability.png}
\caption{Classificazione delle domande per robustezza alla permutazione: l'83.5\% mostra Position Bias, solo il 15.7\% è Robust e lo 0.7\% è Stable.}
\label{fig:perm_stability}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.78\textwidth]{fig2c_threat_resistance.png}
\caption{Resistenza alla minaccia testuale: l'80.7\% delle domande rimane Stable, l'8.8\% migliora (Improved), il 10.4\% collassa.}
\label{fig:threat_resistance}
\end{figure}

% ----------------------------------------------------------------------
\subsection{Coerenza Log-Testo}
% ----------------------------------------------------------------------

La coerenza log-testo misura se la scelta più probabile secondo i logit del modello (intenzione ``matematica'') coincide con l'opzione espressa nella risposta generata testualmente.

\paragraph{Risultati:}
\begin{itemize}
    \item La coerenza media è \textbf{19.9\%}, coincidente con il validity rate --- ciò indica una correlazione perfetta ($r = 1.0$) tra validità e coerenza: quando il modello produce una risposta valida, la scelta testuale coincide \emph{sempre} con la scelta logit.
    \item L'80.1\% delle domande ha consistency = 0 (il modello non genera una risposta valida).
    \item Questo risultato conferma che per Pythia-160M il problema principale non è l'incoerenza tra logit e testo, ma la \textbf{incapacità di produrre risposte nel formato atteso}.
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{fig3a_log_consistency_hist.png}
\caption{Distribuzione della coerenza log-testo per domanda. Il pattern binario (0 o 1) riflette la struttura del validity rate.}
\label{fig:log_consistency}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{fig3b_log_consistency_by_area.png}
\caption{Coerenza log-testo per macro area tematica. Aree con risposte umane pi\`u strutturate tendono a mostrare coerenza maggiore; la variabilit\`a intra-area riflette la diversit\`a delle domande.}
\label{fig:log_consistency_by_area}
\end{figure}

% ----------------------------------------------------------------------
\subsection{Allineamento Umano}
% ----------------------------------------------------------------------

L'alignment score $\mathcal{A}$ misura quanto la distribuzione del modello si avvicina a quella della popolazione umana totale (Pew survey):
$$
\mathcal{A} = 1 - \frac{\text{WD}(D_{\text{LLM}}, D_{\text{Human}})}{\text{WD}_{\max}}
$$
dove $\text{WD}_{\max} = N_{\text{opzioni}} - 1$ è la massima distanza possibile.

\paragraph{Osservazioni:}
\begin{itemize}
    \item L'alignment medio è \textbf{0.652} (mediana 0.668), con ampia variabilità tra domande (min 0.105, max 0.999, $\sigma = 0.190$).
    \item Il modello si allinea meglio su aree tematiche con chiare preferenze maggioritarie: le macro aree con mediana più alta sono \textbf{Health} (0.759), \textbf{Economy} (0.742), \textbf{Environment} (0.721), \textbf{Institutional Confidence} (0.714).
    \item Su topic polarizzati (es. armi, aborto, immigrazione), l'allineamento è debole, riflettendo la difficoltà di un modello base a catturare la complessità delle opinioni umane.
    \item L'alignment è calcolato rispetto alla \textbf{popolazione totale} del survey Pew, senza filtrare per sottogruppi demografici.
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=0.78\textwidth]{fig4a_alignment_hist.png}
\caption{Distribuzione dell'alignment score (1 = perfetto allineamento con le risposte umane). La media (0.652) e la mediana (0.668) indicano un allineamento moderato.}
\label{fig:alignment_hist}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.88\textwidth]{fig4b_alignment_by_area.png}
\caption{Alignment score per macro area tematica. Le aree con risposte umane più ``consensuali'' (Health, Economy) mostrano allineamento migliore; aree polarizzate (Politics, Religion) hanno punteggi più bassi.}
\label{fig:alignment_by_area}
\end{figure}

% ----------------------------------------------------------------------
\subsection{Coerenza Politica}
% ----------------------------------------------------------------------

Per ogni domanda, abbiamo identificato il sottogruppo demografico con \textbf{Wasserstein Distance minima} rispetto alla distribuzione del modello. La tabella seguente mostra la frequenza dei "vincitori" su 380 topic:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Gruppo Demografico} & \textbf{N. Topic Allineati} & \textbf{Percentuale} \\
\midrule
Democrat      & 104 & 27.4\% \\
Liberal       & 103 & 27.1\% \\
Republican    &  95 & 25.0\% \\
Conservative  &  29 &  7.6\% \\
Moderate      &  26 &  6.8\% \\
Independent   &  23 &  6.1\% \\
\bottomrule
\end{tabular}
\caption{Distribuzione dei topic per gruppo demografico più allineato.}
\end{table}

\begin{figure}[ht]
\centering
\includegraphics[width=0.58\textwidth]{fig5a_political_pie.png}
\caption{Affinità politica del modello per domanda. I colori riflettono lo spettro politico USA: Democrat/Liberal (blu), Republican/Conservative (rosso), Moderate/Independent (grigio).}
\label{fig:political_pie}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{fig5b_political_topics_bar.png}
\caption{Numero di topic allineati per gruppo demografico. Democrat e Liberal dominano con 104 e 103 topic rispettivamente.}
\label{fig:political_topics_bar}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.92\textwidth]{fig5c_wd_heatmap.png}
\caption{Heatmap della Wasserstein Distance media per macro area $\times$ gruppo demografico. Valori bassi (verde) indicano maggiore allineamento. Si nota un allineamento differenziato per area tematica.}
\label{fig:wd_heatmap}
\end{figure}

\paragraph{Risultati chiave:}
\begin{enumerate}
    \item \textbf{Bias liberal/democratico}:
    Sommando Democrat + Liberal otteniamo \textbf{54.5\% dei topic}, contro il 32.6\% di Republican + Conservative. Questo conferma il risultato di Santurkar et al.: anche un modello base non allineato (Pythia-160M, addestrato su The Pile senza RLHF) mostra una tendenza verso opinioni liberal/democratiche.
    
    \item \textbf{Minore polarizzazione rispetto a GPT-3/4}:
    Rispetto ai modelli analizzati da Santurkar et al., Pythia-160M mostra un bias \emph{meno marcato}. GPT-3 e GPT-4, sottoposti ad alignment con feedback umani prevalentemente liberal, raggiungono allineamenti >80\% con Democrat/Liberal. Pythia-160M, pur privilegiando questi gruppi, mantiene una distribuzione più equilibrata.
    
    \item \textbf{Consistency score variabile}:
    Il consistency\_score per topic (presente nel report aggregato) misura quanto il modello si allinea \emph{coerentemente} con lo stesso gruppo su domande correlate. Valori bassi ($< 0.5$) indicano incoerenza --- il modello "cambia idea" tra domande dello stesso argomento. Questo riflette la mancanza di una "voce" politica stabile, tipica dei modelli non ottimizzati.
\end{enumerate}

\paragraph{Confronto con Santurkar et al.:}
\begin{itemize}
    \item \textbf{Santurkar et al.}: GPT-3 Davinci allineato $\rightarrow$ 89\% Democrat, 7\% Republican
    \item \textbf{Questo studio}: Pythia-160M base $\rightarrow$ 27.4\% Democrat, 25.0\% Republican
    \item L'RLHF \emph{amplifica drasticamente} il bias preesistente nei dati di pre-training
\end{itemize}

% ----------------------------------------------------------------------
\subsection{Discussione}
% ----------------------------------------------------------------------

\paragraph{Implicazioni:}
I risultati confermano che:
\begin{enumerate}
    \item \textbf{Il bias liberal è intrinseco ai dati}, non solo all'alignment: anche Pythia-160M, mai esposto a feedback umani, privilegia sottogruppi Democrat/Liberal.
    \item \textbf{L'alignment peggiora il problema}: confrontando i nostri risultati (modello base) con quelli di Santurkar et al. (modelli allineati), si osserva che RLHF converte una lieve preferenza in una \emph{schiacciante maggioranza}.
    \item \textbf{Piccoli modelli $\neq$ diverse opinioni}: contrariamente a quanto si potrebbe sperare, ridurre la dimensione del modello non elimina il bias --- semplicemente lo rende meno accurato e più instabile.
\end{enumerate}

\begin{figure}[ht]
\centering
\includegraphics[width=0.82\textwidth]{fig0_summary_table.png}
\caption{Tabella riepilogativa di tutte le metriche calcolate per Pythia-160M. La tabella evidenzia il basso validity rate, il forte position bias e la tendenza liberal/democratica.}
\label{fig:summary_table}
\end{figure}

\paragraph{Limitazioni dello studio:}
\begin{itemize}
    \item \textbf{Basso validity rate}: con solo il 19.9\% di risposte valide, l'analisi è basata su un sottoinsieme limitato di comportamenti del modello.
    \item \textbf{Modello di piccole dimensioni}: Pythia-160M è un proof-of-concept; per risultati più robusti sarebbe necessario testare modelli più grandi (Pythia-1B, 6.9B, ecc.).
    \item \textbf{Position bias dominante}: la forte dipendenza dall'ordine delle opzioni potrebbe mascherare alcune preferenze intrinseche.
\end{itemize}

% ======================================================================
\section{Conclusioni e Sviluppi Futuri}
% ======================================================================

Questo studio ha replicato con successo l'analisi di Santurkar et al. su un modello base open-source, confermando la presenza di \textbf{bias liberali intrinseci} nei Large Language Models anche in assenza di alignment esplicito. 

\paragraph{Conclusioni:}
\begin{itemize}
    \item I modelli base (Pythia-160M) mostrano già una preferenza verso opinioni Democrat/Liberal (\textasciitilde54\% dei topic), sebbene meno marcata rispetto ai modelli allineati (GPT-3: 89\%)
    \item L'RLHF amplifica drammaticamente i bias preesistenti, rendendo i modelli meno rappresentativi della diversità di opinioni della popolazione
    \item La robustezza a perturbazioni è limitata: position bias e basso validity rate caratterizzano i modelli non ottimizzati
\end{itemize}

\paragraph{Sviluppi futuri:}
\begin{enumerate}
    \item \textbf{Scaling analysis}: eseguire l'analisi comparativa sui 5 modelli Pythia (160M, 1B, 1.4B, 2.8B, 6.9B) per quantificare l'effetto della dimensione sul bias politico, sulla robustezza e sull'allineamento umano
    \item \textbf{Modelli alternativi}: testare LLaMA, Mistral, GPT-2 e altri modelli open-source con diverse strategie di pre-training per identificare pattern comuni
    \item \textbf{Effect of alignment}: confrontare modelli base vs. istruiti (es. Pythia vs. Pythia-Chat) per isolare quantitativamente l'impatto dell'RLHF
    \item \textbf{Debiasing techniques}: sperimentare prompt engineering, fine-tuning su dataset bilanciati, e constitutional AI per mitigare i bias
    \item \textbf{Cross-cultural analysis}: replicare l'analisi su survey non-USA (es. European Social Survey) per verificare se il bias liberal sia specifico del contesto americano
    \item \textbf{Temporal dynamics}: analizzare checkpoints intermedi di Pythia per studiare l'emergenza del bias durante il training
\end{enumerate}

\medskip
\noindent In conclusione, questo lavoro sottolinea l'importanza della \textbf{trasparenza} nella documentazione dei bias dei LLM e della necessità di sviluppare modelli più rappresentativi della diversità di opinioni umane, specialmente in contesti sensibili come informazione politica, sanità, e giustizia.

% ======================================================================
\section*{Ringraziamenti}
% ======================================================================

Questo progetto è stato sviluppato nell'ambito del corso di \emph{Deep Learning e Architetture di Rete} (A.A. 2024/2025). Si ringrazia il Prof. [Nome Docente] per le preziose indicazioni metodologiche e il supporto durante l'implementazione. Un ringraziamento speciale al Pew Research Center per aver reso pubblicamente disponibili i dataset dell'American Trends Panel, e agli autori di Santurkar et al. per aver condiviso codice e risorse dell'OpinionQA dataset.

\medskip
\noindent \textbf{Codice e riproducibilità:} Tutti gli script Python utilizzati in questo studio sono disponibili nella sottocartella \texttt{dataset\_e\_script/progetto/script/}. Gli esperimenti sono stati eseguiti su [specificare hardware, es. CPU Intel i7 / GPU NVIDIA RTX 3060] con PyTorch 2.x e Transformers 4.x.

\newpage
% ======================================================================
% BIBLIOGRAFIA
% ======================================================================
\begin{thebibliography}{9}

\bibitem{santurkar2023whose}
Santurkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P., \& Hashimoto, T. (2023).
\emph{Whose Opinions Do Language Models Reflect?}
Proceedings of the 40th International Conference on Machine Learning (ICML).

\bibitem{hartmann2024political}
Hartmann, J., Schwenzow, J., \& Witte, M. (2024).
\emph{The Political Compass of Large Language Models}.
Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL).

\bibitem{biderman2023pythia}
Biderman, S., Schoelkopf, H., Anthony, Q., et al. (2023).
\emph{Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}.
Proceedings of the 40th International Conference on Machine Learning (ICML).

\bibitem{pewresearch}
Pew Research Center.
\emph{American Trends Panel}.
\url{https://www.pewresearch.org/american-trends-panel-datasets/}.

\end{thebibliography}

\end{document}
